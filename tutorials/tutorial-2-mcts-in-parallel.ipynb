{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T18:26:53.015218Z",
     "start_time": "2024-05-01T18:26:51.583848Z"
    }
   },
   "source": "%pip install --upgrade pip",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T18:26:56.328654Z",
     "start_time": "2024-05-01T18:26:55.515699Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install gevent",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T18:26:58.425445Z",
     "start_time": "2024-05-01T18:26:58.415038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gevent import monkey\n",
    "monkey.patch_all() # required for gevent to work properly in Jupyter notebooks"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T18:27:51.589457Z",
     "start_time": "2024-05-01T18:27:50.064961Z"
    }
   },
   "source": [
    "%pip install --upgrade celery[redis] numpy scipy numba matplotlib tqdm ipympl ffmpeg watermark circuitree\n",
    "%load_ext watermark\n",
    "seed = 2024"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel MCTS with CircuiTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCTS is an iterative sampling algorithm, where the reward found in each iteration affects sampling in later iterations. While perfect parallel execution isn't possible, we can achieve quite good performance using the so-called lock-free method [[1]](https://doi.org/10.1007/978-3-642-12993-3_2), where multiple multiple search threads in the same CPU (the *main node*) are running MCTS concurrently, each one taking turns editing the search graph. We will implement this in detail later in the tutorial, but in brief, instead of computing the (usually expensive) reward function, each search thread on the main node sends a request to a group of worker CPUs (the *worker node*) somewhere else that will do the actual computation, and while that thread is waiting for the result, other search threads can use the main CPU. As long as our execution time is significantly longer than the time spent sending and receiving those signals, we should see a performance boost!\n",
    "\n",
    "First let's watch a parallel search in action using an example case, a parallelized version of the \"bistability\" circuit search from Tutorial 1. Here, we will make each reward evaluation take 0.1 seconds longer by setting the flag `expensive=True`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T18:27:44.139010Z",
     "start_time": "2024-05-01T18:27:43.710222Z"
    }
   },
   "source": [
    "from tutorial_2_parallel_example import ParallelBistabilityTree\n",
    "from time import perf_counter\n",
    "\n",
    "# Create the tree search object\n",
    "tree = ParallelBistabilityTree(root=\"ABC::\")\n",
    "\n",
    "# # Run the search sequentially, with an expensive reward function (17 minutes)\n",
    "# tree.search_mcts(\n",
    "#     n_steps=10_000, run_kwargs=dict(expensive=True)  \n",
    "# )\n",
    "\n",
    "# # Search in parallel with 50 threads (ideally, 64x faster = 16 seconds)\n",
    "# start_time = perf_counter()\n",
    "# tree.search_mcts_parallel(\n",
    "#     n_steps=10_000, n_threads=64, run_kwargs=dict(expensive=True)\n",
    "# )\n",
    "# end_time = perf_counter()\n",
    "\n",
    "# print(\"Done!\")\n",
    "# print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel CircuiTree on a single machine\n",
    "\n",
    "In order to parallelize the search on a local machine, we can nominate a group of CPUs in our own computer to be the worker node that performs reward function evaluations. We can coordinate the main and worker nodes using a *producer-consumer* queue. The main node will produce tasks (calls to the reward function) that get added to the queue, and the worker node will consume tasks from the queue and return the result to a shared database where the main node can look up the result. We'll manage this task queue with the Python utility `celery`. \n",
    "\n",
    "Here's a schematic of how that infrastructure looks.\n",
    "\n",
    "![Local-Infrastructure](./local_parallel_infrastructure.png)\n",
    "\n",
    "### The 4 steps to setting up a local parallel search\n",
    "1) Set up a simple database.\n",
    "2) Package the reward function into a `celery` app.\n",
    "3) Define a `CircuiTree` subclass that calls the reward function in (2).\n",
    "4) Launch some workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Database installation\n",
    "\n",
    "We will be using a lightweight database called Redis (https://redis.io/).\n",
    "\n",
    "If you are running this notebook on Colab or on a machine without Redis installed, you can uncomment and run the next code block to install Redis. Otherwise, please skip the next code block and follow the installation instructions [here](https://redis.io/docs/latest/operate/oss_and_stack/install/install-redis/) instead. \n",
    "\n",
    "If you are using a Redis server hosted somewhere else, you can skip the next code block and change the `host` and `port` arguments later in the notebook to point to your server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "####################################################################################\n",
    "### If you are using Colab, uncomment and run this :) ##############################\n",
    "####################################################################################\n",
    "# # Download the latest stable release and make from source (can take a few minutes)\n",
    "# !curl -o ./redis-server.tar.gz -fsSL https://download.redis.io/redis-stable.tar.gz\n",
    "# !tar -xf ./redis-server.tar.gz\n",
    "# !cd ./redis-stable && make # Can take a while (5+ minutes)\n",
    "# !/content/redis-stable/src/redis-server --daemonize yes"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be sure to test your installation!!**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T18:35:08.828166Z",
     "start_time": "2024-05-01T18:35:08.587982Z"
    }
   },
   "source": [
    "## This should return \"PONG\"\n",
    "\n",
    "# Colab notebook users\n",
    "# !/content/redis-stable/src/redis-cli ping \n",
    "\n",
    "# Local installations\n",
    "!redis-cli ping "
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Making a `celery` app with the reward function \n",
    "The app is a Python script that tells `celery` where the database is and which tasks it will be managing. For instance, here is the script for the bistability app."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T18:36:57.539396Z",
     "start_time": "2024-05-01T18:36:57.518999Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(Path(\"tutorial_app.py\").read_text())"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `Celery` command to create an app that uses the `Redis` database to pass messages (the `broker` option) and store results (the `backend` argument). The URL here points to the default location for a local database (port `6379` on the `localhost` network). Any function with the `@app.task` decorator becomes a `celery` *task* that can be executed by a worker - we'll see how this looks in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calling the reward function as a `celery` task\n",
    "\n",
    "Unlike a normal function call, a call to a `celery` task is *asynchronous*. This means that when the main node calls the function, it dispatches a task to the workers, and the result can be requested later. This uses different syntax - instead of running `reward = get_reward(...)` directly, we run `result = get_reward_celery.delay(...)` to dispatch the task from the main node to the workers. This immediately returns an `AsyncResult` object that can be inspected to monitor progress. Then, once we need the result, we call `future.get()` and wait for the reward to arrive. While one thread is waiting for the reply, another thread can take over the main node and run a search iteration. \n",
    "\n",
    "All we need to do in this step is make a new subclass of `CircuiTree` that runs the reward function using the app. Here's what that looks like in our bistability example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "print(Path(\"tutorial_2_parallel_example.py\").read_text())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's `threading` module can manage up to a few dozen threads, but we want to run a search with hundreds to thousands of threads. For this, we will use the `gevent` module, which re-defines many of the built-in Python commands in order to support its highly scalable \"green threads.\" Re-defining built-in code is called \"monkey patching,\" and it has to be run as the first line in the file where we define the class. (We also ran `monkey.patch_all()` at the beginning of this notebook - this is only necessary for notebooks, not for scripts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launching a worker node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can launch a worker node using `celery`'s command line interface. To do so, open a separate terminal, `cd` to the folder with the app, and run the following command, replacing the `XX` with the number of CPUs to use. If you are using a virtual environment, be sure to activate that first. (If you aren't, you should be!)\n",
    "\n",
    "```\n",
    "# Launch a worker with 'XX' CPUs, specifying the app with the `.app` suffix\n",
    "celery --app tutorial_app.app worker --concurrency=XX \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tutorial-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
